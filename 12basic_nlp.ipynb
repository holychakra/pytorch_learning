{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"新的數學方法和概念，常常比解決數學問題本身更重要。\",\n",
    "    \"在數學中，我們發現真理的主要工具是歸納和模擬。\",\n",
    "    \"數學方法滲透並支配著一切自然科學的理論分支。它愈來愈成為衡量科學成就的主要標志了。\",\n",
    "    \"第一是數學，第二是數學，第三是數學。\",\n",
    "    \"歷史使人賢明，詩造成氣質高雅的人，數學使人高尚，自然哲學使人深沈，道德使人穩重，而倫理學和修辭學則使人善於爭論。\"\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Student\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.483 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['新', '的', '數學', '方法', '和', '概念', '，', '常常', '比解', '決數學', '問題', '本身', '更', '重要', '。'], ['在', '數學', '中', '，', '我們', '發現', '真理', '的', '主要', '工具', '是', '歸納', '和', '模擬', '。'], ['數學', '方法', '滲透並', '支配', '著', '一切', '自然科', '學', '的', '理論', '分支', '。', '它', '愈來', '愈成', '為', '衡量', '科學', '成就', '的', '主要', '標志', '了', '。'], ['第一', '是', '數學', '，', '第二', '是', '數學', '，', '第三', '是', '數學', '。'], ['歷史', '使', '人賢明', '，', '詩', '造成', '氣質', '高雅', '的', '人', '，', '數學', '使', '人', '高尚', '，', '自然', '哲學', '使', '人', '深沈', '，', '道德', '使人', '穩重', '，', '而倫理學', '和', '修辭', '學則', '使', '人善', '於', '爭論', '。']]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "words_list = [list(jieba.cut(doc)) for doc in docs]\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'爭論', '一切', '高尚', '問題', '學則', '常常', '滲透並', '詩', '人善', '和', '自然科', '使', '歷史', '主要', '高雅', '它', '中', '成就', '哲學', '分支', '本身', '方法', '重要', '深沈', '穩重', '在', '第三', '的', '支配', '數學', '概念', '衡量', '人', '模擬', '比解', '第一', '自然', '決數學', '工具', '是', '道德', '，', '而倫理學', '愈來', '為', '了', '造成', '更', '科學', '新', '愈成', '學', '理論', '第二', '人賢明', '使人', '發現', '著', '氣質', '修辭', '於', '標志', '歸納', '。', '我們', '真理'}\n",
      "{'爭論', '一切', '高尚', '問題', '學則', '常常', '滲透並', '詩', '人善', '和', '自然科', '使', '歷史', '主要', '高雅', '它', '中', '成就', '哲學', '分支', '本身', '方法', '重要', '深沈', '穩重', '在', '第三', '的', '支配', '數學', '概念', '衡量', '人', '模擬', '比解', '第一', '自然', '決數學', '工具', '是', '道德', '，', '而倫理學', '愈來', '為', '了', '造成', '更', '科學', '新', '愈成', '學', '理論', '第二', '人賢明', '使人', '發現', '著', '氣質', '修辭', '於', '標志', '歸納', '。', '我們', '真理'}\n"
     ]
    }
   ],
   "source": [
    "# build word dictionary\n",
    "vocb = set([word for words in words_list for word in words])\n",
    "print(vocb)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}\n",
    "print(vocb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'衡量': 0, '愈來': 1, '人賢明': 2, '，': 3, '我們': 4, '是': 5, '方法': 6, '深沈': 7, '為': 8, '真理': 9, '著': 10, '在': 11, '歸納': 12, '學': 13, '標志': 14, '常常': 15, '理論': 16, '道德': 17, '它': 18, '數學': 19, '本身': 20, '而倫理學': 21, '工具': 22, '高尚': 23, '科學': 24, '主要': 25, '的': 26, '成就': 27, '歷史': 28, '人': 29, '中': 30, '使': 31, '比解': 32, '自然': 33, '修辭': 34, '滲透並': 35, '人善': 36, '支配': 37, '新': 38, '哲學': 39, '第二': 40, '造成': 41, '高雅': 42, '一切': 43, '氣質': 44, '決數學': 45, '使人': 46, '第三': 47, '詩': 48, '問題': 49, '自然科': 50, '分支': 51, '。': 52, '模擬': 53, '更': 54, '了': 55, '概念': 56, '重要': 57, '第一': 58, '爭論': 59, '和': 60, '愈成': 61, '發現': 62, '穩重': 63, '學則': 64, '於': 65}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '衡量', 1: '愈來', 2: '人賢明', 3: '，', 4: '我們', 5: '是', 6: '方法', 7: '深沈', 8: '為', 9: '真理', 10: '著', 11: '在', 12: '歸納', 13: '學', 14: '標志', 15: '常常', 16: '理論', 17: '道德', 18: '它', 19: '數學', 20: '本身', 21: '而倫理學', 22: '工具', 23: '高尚', 24: '科學', 25: '主要', 26: '的', 27: '成就', 28: '歷史', 29: '人', 30: '中', 31: '使', 32: '比解', 33: '自然', 34: '修辭', 35: '滲透並', 36: '人善', 37: '支配', 38: '新', 39: '哲學', 40: '第二', 41: '造成', 42: '高雅', 43: '一切', 44: '氣質', 45: '決數學', 46: '使人', 47: '第三', 48: '詩', 49: '問題', 50: '自然科', 51: '分支', 52: '。', 53: '模擬', 54: '更', 55: '了', 56: '概念', 57: '重要', 58: '第一', 59: '爭論', 60: '和', 61: '愈成', 62: '發現', 63: '穩重', 64: '學則', 65: '於'}\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "Embedding(66, 200)\n"
     ]
    }
   ],
   "source": [
    "vocb_size = len(vocb)  # 66\n",
    "embedding_size = 200\n",
    "embeds = nn.Embedding(vocb_size, embedding_size) \n",
    "print(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\"數學\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n",
      "tensor([[-7.8346e-01,  9.8232e-01, -1.2130e-01,  2.1810e-01, -6.9415e-02,\n",
      "          1.6978e+00, -2.4325e-01, -2.4060e-01,  1.9885e+00, -2.6022e-02,\n",
      "         -4.1901e-01, -1.3125e-01,  1.1121e+00,  1.2121e-02,  1.4913e+00,\n",
      "          1.5119e-01, -1.2651e-01, -6.3120e-01, -9.4133e-01,  6.7252e-01,\n",
      "         -2.9765e-01, -9.5471e-01,  5.5764e-01, -4.9128e-01, -6.2335e-01,\n",
      "          3.4183e-01,  8.1355e-01, -1.9768e+00,  1.6110e+00,  1.6581e+00,\n",
      "         -1.4435e-01,  4.1550e-01, -1.0098e+00, -1.3490e+00, -1.4543e+00,\n",
      "         -1.4047e+00,  1.1309e+00,  8.7460e-01,  3.4590e-01, -1.9791e+00,\n",
      "         -4.3906e-01, -1.9337e-01,  1.6910e-01, -8.2671e-01,  1.0557e+00,\n",
      "         -4.8144e-01,  4.7116e-01,  6.4566e-01, -3.4474e-01,  4.1806e-03,\n",
      "          1.7940e-01, -1.7304e-01,  1.3098e+00, -5.4056e-01, -1.5260e+00,\n",
      "          2.4461e-01,  1.0570e+00,  3.3756e-01, -5.9510e-01, -3.2188e-01,\n",
      "          1.1966e-01,  1.5973e+00, -7.2256e-01, -2.3459e-01,  4.3433e-01,\n",
      "         -4.8255e-01,  5.2360e-01, -4.3333e-01,  1.1537e-01,  1.1784e+00,\n",
      "          1.1323e+00,  2.0332e-01,  7.1659e-02,  2.1571e+00, -1.6449e+00,\n",
      "         -2.6655e-01,  1.9609e+00, -6.8219e-01,  4.7923e-02,  2.0443e-01,\n",
      "          3.2296e-01,  3.7692e-01,  6.6763e-01,  1.5982e+00, -1.3031e+00,\n",
      "         -1.1634e+00, -8.5894e-01,  1.0489e+00,  4.7502e-01, -1.8494e+00,\n",
      "          4.1655e-01,  2.1417e+00,  4.5739e-01,  8.2912e-01,  5.0189e-01,\n",
      "          2.6040e-01,  3.1746e-01, -2.4695e-01,  8.1907e-01,  4.4588e-03,\n",
      "          1.1032e-01, -1.3201e-01, -1.3457e+00, -1.3295e-01, -8.9153e-01,\n",
      "          1.2424e+00, -1.3648e+00, -8.8645e-02, -2.2040e+00, -1.8808e+00,\n",
      "          1.4432e+00, -1.4092e+00,  1.0367e+00,  1.9905e+00, -5.9250e-01,\n",
      "          7.4089e-01,  1.5872e+00,  8.0048e-01, -7.3154e-01,  8.8938e-01,\n",
      "          3.4673e-01,  4.2716e-01, -6.8468e-01, -1.4471e+00, -9.9540e-01,\n",
      "         -7.8289e-01, -3.2199e-01,  7.1963e-01, -1.1910e+00, -8.1528e-01,\n",
      "         -4.0680e-02, -8.6405e-01,  9.3903e-01, -5.2862e-01,  1.5536e-01,\n",
      "         -1.0465e+00, -3.2388e-01,  7.4021e-01, -2.1050e+00, -6.3744e-01,\n",
      "         -8.5986e-01,  1.4807e+00, -1.0699e-01, -2.3503e-02, -2.6488e-01,\n",
      "          2.6227e-01,  1.5908e-03,  8.9520e-01, -8.5580e-01, -6.0036e-01,\n",
      "          6.5661e-01,  1.2924e+00,  1.7026e+00,  1.8330e+00,  7.3358e-01,\n",
      "         -9.7185e-02, -3.8204e-02, -1.3058e+00, -1.5991e+00, -1.6857e-01,\n",
      "         -1.8143e+00,  1.7861e-01,  1.4059e+00,  1.8554e-01,  6.9353e-01,\n",
      "          7.0085e-01,  1.4272e-01,  7.8110e-01, -7.6115e-01,  2.6338e-01,\n",
      "         -2.1180e-01, -3.6873e-01,  4.2837e-01,  6.5142e-01,  5.6741e-01,\n",
      "          6.3794e-01,  1.0029e+00,  1.6494e+00, -2.3223e-01,  5.8885e-02,\n",
      "         -1.6416e+00,  1.7385e-01, -1.8036e-04,  6.3537e-01,  9.8331e-01,\n",
      "         -2.1147e-01, -1.0025e+00, -8.8104e-01, -1.3678e+00, -7.5790e-02,\n",
      "         -4.6335e-01,  1.2308e+00,  1.9441e-01,  2.6394e+00, -1.5587e+00,\n",
      "          1.6344e+00,  1.2316e+00, -7.8273e-01, -7.1432e-01, -1.0842e+00]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "look_up = torch.LongTensor([word_to_idx[\"數學\"]])\n",
    "print(embeds(look_up).shape)\n",
    "print(embeds(look_up))\n",
    "\n",
    "# 還沒train過 其實這向量沒有意義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pre-trained word2vect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# download the model and return as object ready for use\n",
    "word2vec_model = api.load(\"glove-twitter-25\")\n",
    "#word2vec_model = gensim.models.KeyedVectors.load(\"news.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193514, 25)\n"
     ]
    }
   ],
   "source": [
    "embedding_shape = word2vec_model.vectors.shape\n",
    "print(embedding_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "#initialized embedding layer\n",
    "embed = nn.Embedding(embedding_shape[0], embedding_shape[1])\n",
    "\n",
    "#copy pre-trained word2vect model\n",
    "embed.weight.data.copy_(torch.from_numpy(word2vec_model.vectors))\n",
    "\n",
    "#freeze weight\n",
    "embed.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 25])\n",
      "tensor([[-1.2304,  0.4831,  0.1410, -0.0295, -0.6525, -0.1855,  2.1033,  1.7516,\n",
      "         -1.3001, -0.3211, -0.8477,  0.4200, -3.8823,  0.1964, -0.7286, -0.8527,\n",
      "          0.2317, -1.0763, -0.8302,  0.1081, -0.5102,  0.2769, -1.1895,  0.9809,\n",
      "         -0.1396]])\n"
     ]
    }
   ],
   "source": [
    "index = word2vec_model.key_to_index[\"happy\"]\n",
    "#查表\n",
    "print(embed(torch.LongTensor([index])).shape)\n",
    "print(embed(torch.LongTensor([index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20555 ,  0.14291 ,  0.72309 , -0.82082 , -0.48214 ,  0.36303 ,\n",
       "       -0.348   ,  2.2456  , -1.0749  , -0.79381 ,  0.33621 , -2.4247  ,\n",
       "       -0.44681 , -0.70426 ,  1.3172  ,  0.48233 ,  0.074172, -0.15488 ,\n",
       "        0.40502 , -0.29675 ,  0.89252 , -2.0936  ,  0.75108 ,  0.44415 ,\n",
       "        0.83416 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.vectors[5160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 225, 117, 635], [6111, 656]]\n"
     ]
    }
   ],
   "source": [
    "docs = [[\"i\",\"am\",\"good\",\"guy\"],[\"delicious\",\"food\"]]\n",
    "\n",
    "for doc in docs:\n",
    "    for word in doc:\n",
    "        word2vec_model.key_to_index[word]\n",
    "indexes = [[word2vec_model.key_to_index[word] for word in doc] for doc in docs]\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([10, 225, 117, 635]) list([6111, 656])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_indexes = (np.array(indexes))\n",
    "print(np_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('AI_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17351a460c7b6a87bea396add1443477c853166d48a66f1614cf7bf03cffca4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
